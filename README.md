# **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ3**
### **–ó–∞–¥–∞–Ω–∏–µ ‚Ññ1**
```
import re
import unicodedata


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç:
    - –ü—Ä–∏–≤–æ–¥–∏—Ç –∫ casefold (–ª—É—á—à–µ —á–µ–º lower –¥–ª—è –Æ–Ω–∏–∫–æ–¥–∞)
    - –ó–∞–º–µ–Ω—è–µ—Ç –±—É–∫–≤—ã —ë/–Å –Ω–∞ –µ/–ï
    - –£–±–∏—Ä–∞–µ—Ç –Ω–µ–≤–∏–¥–∏–º—ã–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã, –∑–∞–º–µ–Ω—è—è –∏—Ö –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    - –°—Ö–ª–æ–ø—ã–≤–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
    """
    if casefold:
        text = text.casefold()
    
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    
    # –ó–∞–º–µ–Ω—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = ''.join(char if char.isprintable() or char.isspace() else ' ' for char in text)
    
    # –°—Ö–ª–æ–ø—ã–≤–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
    text = re.sub(r'\s+', ' ', text)
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    return text.strip()
print('normalize')
print("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t ->", normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print("\"—ë–∂–∏–∫, –Å–ª–∫–∞\" ->", normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print("\"Hello\r\nWorld\" ->", normalize("Hello\r\nWorld"))
print("\"  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  \" ->", normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


def tokenize(text: str) -> list[str]:
    
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ + –¥–µ—Ñ–∏—Å –≤–Ω—É—Ç—Ä–∏)
    pattern = r'[\w]+(?:-[\w]+)*'
    tokens = re.findall(pattern, text)
    
    return tokens
print('tokenize')
print("\"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä\" ->", tokenize(("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")))
print("\"hello,world!!!\" ->", tokenize(("hello,world!!!")))
print("\"–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ\" ->", tokenize(("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")))
print("\"2025 –≥–æ–¥\" ->", tokenize(("2025 –≥–æ–¥")))
print("\"emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ\" ->", tokenize(("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ")))

def count_freq(tokens: list[str]) -> dict[str, int]:
    """
    –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤
    """
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq



def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã
    –ü—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ —á–∞—Å—Ç–æ—Ç - –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    """
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –∑–∞—Ç–µ–º –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Å–ª–æ–≤–∞ (–∞–ª—Ñ–∞–≤–∏—Ç—É)
    sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
    return sorted_items[:n]
print('count_freq + top_n')
print('["a","b","a","c","b","a"] ->', count_freq((["a","b","a","c","b","a"])))
print('top_n(..., n=2) ->', top_n(count_freq((["a","b","a","c","b","a"]))))
print('["bb","aa","bb","aa","cc"] ->', count_freq((["bb","aa","bb","aa","cc"])))
print('top_n(..., n=2) ->', top_n(count_freq((["bb","aa","bb","aa","cc"]))))
```
![](images/lab03/text.jpg)

### **–ó–∞–¥–∞–Ω–∏–µ ‚Ññ2**
```
from text import *
def text_stats(n: str):
    n=input()
    norm= normalize(n)
    tokens=tokenize(norm)
    freq=count_freq(tokens)
    print(f'–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}')
    print(f'–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(freq)}')
    print(f'–¢–æ–ø-5:')
    top=top_n(freq)
    for word, value in top:
```
![](images/lab03/text_stats.jpg)

